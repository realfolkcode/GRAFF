{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNSFOkLgWN02KjzvWrXRgvo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/realfolkcode/GRAFF/blob/main/GRAFF_Tutorial_PyG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: *GNNs as Gradient Flows*\n",
        "\n",
        "[Original paper](https://arxiv.org/pdf/2206.10991.pdf) (Di Giovanni, Francesco, et al. \"Graph Neural Networks as Gradient Flows: understanding graph convolutions via energy\")\n",
        "\n",
        "[Michael Bronstein's blogpost](https://towardsdatascience.com/graph-neural-networks-as-gradient-flows-4dae41fb2e8a)\n",
        "\n",
        "In this tutorial, we implement GRAFF (Gradient Flow Framework) using [PyG](https://www.pyg.org/), a pure PyTorch library for GNNs.\n",
        "\n",
        "$\\newcommand{\\matr}[1]{\\mathbf{#1}}$\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ayqfz_rDIkhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A brief theoretical overview\n",
        "\n",
        "When designing a GNN, one could face the following potential problems:\n",
        "- **Over-smoothing** of node features with the increase of the model depth \n",
        "- Poor performance on **heterophilic** data (i.e., neighboring nodes are vastly dissimilar)\n",
        "\n",
        "GRAFF alleviates these issues by viewing GNNs as gradient flows and carefully parametrizing the weight matrix.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NYNnEqJhIuHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ODE perspective\n",
        "\n",
        "Let us denote the node feature matrix by $\\matr{F}$, and normalized adjacency matrix by $\\bar{\\matr{A}} := \\matr{D}^{-\\frac12} \\matr{A} \\matr{D}^{-\\frac12}$, where $\\matr{D}$ contains the node degrees on its diagonal.\n",
        "\n",
        "GNNs (with residual connections) can be seen as the discretizations of differential equations that govern the evolution of node features $\\matr{F}$ given a graph $G$ (possibly with the addition of self-loops):\n",
        "\n",
        "$$\\dot{\\matr{F}}(t) = \\operatorname{GNN}_{\\theta(t)}(G, \\matr{F}(t)), \\quad \\matr{F}(0) = \\matr{F},$$\n",
        "\n",
        "where $\\operatorname{GNN}_{\\theta(t)}$ is a GNN layer at time $t$ with parameters $\\theta(t)$. \n",
        "\n",
        "For example, a residual GCN corresponds to the following Euler discretization:\n",
        "\n",
        "$$\\matr{F}(t + 1) = \\matr{F}(t) + \\sigma\\left( \\bar{\\matr{A}} \\matr{F}(t) \\matr{W}_t \\right),$$\n",
        "\n",
        "where $\\sigma$ is a non-linearity, and $\\matr{W}_t$ is the channel-mixing matrix of layer $t$.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1TjSU6cFZYz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Energy perspective\n",
        "\n",
        "Gradient flows are *the class of differential equations that minimize the energy functional*. If the energy is parametrized with the set of parameters $\\theta$, then the gradient flow of $\\mathcal{E}_{\\theta}$ is defined as follows:\n",
        "\n",
        "$$\\dot{\\matr{F}}(t) = -\\nabla \\mathcal{E}_{\\theta}(\\matr{F}(t))$$\n",
        "\n",
        "Let $\\matr{f}_i$ denote the transposed $i$-th row in $\\matr{F}$ (such that the feature vector of node $i$ is now a column vector).\n",
        "\n",
        "GRAFF considers a class of energies that can be decomposed into \n",
        "- edge-agnostic components $\\mathcal{E}_{\\matr{\\Omega}}^{\\textrm{ext}}$ (\"external field\")\n",
        "- pairwise interactions $\\mathcal{E}_{\\matr{W}}^{\\textrm{pair}}$\n",
        "- the source terms $\\mathcal{E}_{\\tilde{\\matr{W}}}^{\\textrm{source}}$\n",
        "\n",
        "$$\\mathcal{E}_{\\theta}(\\matr{F}) = \\underbrace{\\frac12 \\sum_i \\left< \\matr{f}_i, \\matr{\\Omega} \\matr{f}_i \\right>}_{\\mathcal{E}_{\\matr{\\Omega}}^{\\textrm{ext}}} - \\underbrace{\\frac12 \\sum_{i,j} \\bar{\\matr{A}}_{ij} \\left< \\matr{f}_i, \\matr{W} \\matr{f}_j \\right>}_{\\mathcal{E}_{\\matr{W}}^{\\textrm{pair}}} + \\underbrace{\\sum_i \\left< \\matr{f}_i, \\tilde{\\matr{W}} \\matr{f}_i(0) \\right>}_{\\mathcal{E}_{\\tilde{\\matr{W}}}^{\\textrm{source}}},$$\n",
        "\n",
        "where $\\matr{\\Omega}, \\matr{W}, \\tilde{\\matr{W}}$ are learnable *square* matrices. \n",
        "\n",
        "Differentiating the energy yields an equivalent *gradient flow* formulation, where $\\matr{\\Omega}$ and $\\matr{W}$ are **symmetric**:\n",
        "\n",
        "$$\\dot{\\matr{F}}(t) = -\\matr{F}(t) \\matr{\\Omega} + \\bar{\\matr{A}} \\matr{F}(t) \\matr{W} - \\matr{F}(0) \\tilde{\\matr{W}}$$\n",
        "\n",
        "The Euler discretization:\n",
        "\n",
        "$$\\matr{F}(t + \\tau) = \\matr{F}(t) + \\tau \\left( -\\matr{F}(t) \\matr{\\Omega} + \\bar{\\matr{A}} \\matr{F}(t) \\matr{W} - \\matr{F}(0) \\tilde{\\matr{W}} \\right) $$\n",
        "\n",
        "Note how the middle term $\\bar{\\matr{A}} \\matr{F}(t) \\matr{W}$ is reminiscent to the GCN dynamics. One important difference is that the learnable parameters $\\matr{\\Omega}, \\matr{W}, \\tilde{\\matr{W}}$ are *shared* across all the layers.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qpQOVFUUZf2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What makes GRAFF great\n",
        "\n",
        "The energy functional can be rearranged by highlighting the positive and negative eigenvalues of the channel-mixing matrix $\\matr{W}$ separately. In short, the authors show that the spectrum of $\\matr{W}$ encodes the \"mood\" of edge-wise interactions in a graph:\n",
        "\n",
        "- Positive eigenvalues make interactions *attractive* ðŸ¤—, i.e., neighboring nodes become *similar* (corresponds to magnifying the *low* frequencies, *homophilic* scenario) \n",
        "- Negative eigenvalues make Interactions *repulsive* ðŸ˜’, i.e. neighboring nodes become *dissimilar* (corresponds to magnifying the *high* frequencies, *heterophilic* scenario)\n",
        "\n",
        "Further, they show that without the residual connections, GRAFF is limited only to the homophilic setting, and is vulnerable to over-smoothing. Hence, the residual connections are crucial if we want good performance on both settings."
      ],
      "metadata": {
        "id": "1TIJI_KWHM0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice"
      ],
      "metadata": {
        "id": "PShNdPTqgeXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and Import"
      ],
      "metadata": {
        "id": "LRhWp90fKwly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV97UXNFF7-R",
        "outputId": "5ec67b88-e876-46d9-ab54-c2b94212b12c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-${TORCH}.html"
      ],
      "metadata": {
        "id": "3b1JceiWGVGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Parameter\n",
        "import torch.nn.utils.parametrize as parametrize\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, degree, homophily\n",
        "\n",
        "from torch_geometric.datasets import WebKB, Planetoid\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "PISloqxuFM_j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCN Implementation\n",
        "\n",
        "First, we start by implementing a GCN layer. We slightly modify the example provided in the [Creating Message Passing Layers](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html) tutorial.\n",
        "\n",
        "Recall that GCN is defined as\n",
        "\n",
        "$$\\begin{align}\n",
        "\\matr{F}(t + 1) &= \\sigma\\left( \\bar{\\matr{A}} \\matr{F}(t) \\matr{W}_t \\right) \\\\\n",
        "&= \\sigma \\left( \\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{D_{ii} \\cdot D_{jj}}} \\cdot \\left( \\matr{W}_t \\cdot \\matr{f}_j(t) \\right) \\right)\n",
        "\\end{align}$$\n",
        "\n",
        "The first equation is given in a matrix form, whereas the second equation leverages the *message passing* formulation which encompasses a broader class of graph convolutions. **PyG** provides the `MessagePassing` class, from which we can inherit to implement our layer.\n",
        "\n",
        "In the code, `x` denotes features $\\matr{F}(t) \\in \\mathbb{R}^{n \\times in}$. The result of convolution, $\\matr{F}(t+1)$, has the dimensionality of $n \\times out$. "
      ],
      "metadata": {
        "id": "cDIXp5rH_S9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7wVajE0OFBfW"
      },
      "outputs": [],
      "source": [
        "class GCNConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__(aggr='add')\n",
        "        self.W = Linear(in_channels, out_channels, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.W.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Add self-loops to the adjacency matrix.\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        # Linearly transform node feature matrix.\n",
        "        x = self.W(x)\n",
        "\n",
        "        # Compute normalization.\n",
        "        row, col = edge_index\n",
        "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        # Start propagating messages.\n",
        "        out = self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        # Normalize node features.\n",
        "        return norm.view(-1, 1) * x_j"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define our network as a stacking of two GCN layers with the ReLU non-linearity."
      ],
      "metadata": {
        "id": "uJwF_MR6zq7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNNet(torch.nn.Module):\n",
        "    def __init__(self, dataset, num_hidden):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_features, num_hidden)\n",
        "        self.conv2 = GCNConv(num_hidden, dataset.num_classes)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.conv2.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "hS_QkbLO3QgQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRAFF Implementation\n",
        "\n",
        "Let us summarize the main differences of GRAFF compared to GCN:\n",
        "\n",
        "- All the learnable parameters are shared across the layers\n",
        "\n",
        "- Besides the weight matrix $\\matr{W}$ (pairwise interactions), there are also learnable matrices $\\matr{\\Omega}$ (\"external field\") and $\\tilde{\\matr{W}}$ (\"source\")\n",
        "\n",
        "- $\\matr{\\Omega}, \\matr{W} \\in \\mathbb{R}^{d \\times d}$ and are symmetric\n",
        "\n",
        "- It takes the initial feature matrix $\\matr{F}(0)$ as an additional argument\n",
        "\n",
        "We can keep the message passing logic of GCN intact. Since all the weights are shared, we do not initialize them inside a layer. Instead, we pass the already initialized linear layers as arguments `ext_lin` ($\\matr{\\Omega}$), `pair_lin` ($\\matr{W}$), `source_lin` ($\\tilde{\\matr{W}}$). Actually, we can reuse the same layer again and again because it does not contain any parameters. In this tutorial, we pass the weights as arguments to first give a high-level look at GRAFF.\n",
        "\n",
        "Another difference is that we make the addition of self-loops optional."
      ],
      "metadata": {
        "id": "kDJnBaK0g5en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRAFFConv(MessagePassing):\n",
        "    def __init__(self, ext_lin, pair_lin, source_lin, self_loops=True):\n",
        "        super().__init__(aggr='add')\n",
        "        self.ext_lin = ext_lin\n",
        "        self.pair_lin = pair_lin\n",
        "        self.source_lin = source_lin\n",
        "        self.self_loops = self_loops\n",
        "\n",
        "    def forward(self, x, edge_index, x0):\n",
        "        # (Optionally) Add self-loops to the adjacency matrix.\n",
        "        if self.self_loops:\n",
        "            edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        # Linearly transform node feature matrix.\n",
        "        out = self.pair_lin(x)\n",
        "\n",
        "        # Compute normalization.\n",
        "        row, col = edge_index\n",
        "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        # Start propagating messages.\n",
        "        out = self.propagate(edge_index, x=out, norm=norm)\n",
        "\n",
        "        # Add the external and source contributions\n",
        "        out -= self.ext_lin(x) + self.source_lin(x0)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        # Normalize node features.\n",
        "        return norm.view(-1, 1) * x_j"
      ],
      "metadata": {
        "id": "kAZ3egPoKkyD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While there are many variants of gradient flow parametrizations, we are going to focus on the GRAFF with *diagonally dominant* $\\matr{W}$:\n",
        "\n",
        "$$\\matr{F}(t + \\tau) = \\matr{F}(t) + \\tau \\sigma \\left( -\\matr{F}(t) \\operatorname{diag}(\\matr{\\omega}) + \\bar{\\matr{A}} \\matr{F}(t) \\matr{W} - \\beta \\matr{F}(0) \\right)$$\n",
        "\n",
        "Here, we have\n",
        "- $\\matr{\\Omega} := \\operatorname{diag}(\\matr{\\omega})$, where $\\matr{\\omega} \\in \\mathbb{R}^{d}$\n",
        "- $\\matr{W} := \\matr{W^0} + \\operatorname{diag}(\\matr{w})$, where $\\matr{W}^0$ is symmetric with zero diagonal, and $\\matr{w}$ defined by $\\matr{w}_{\\alpha} = q_{\\alpha} \\sum_{\\beta} | \\matr{W}^0_{\\alpha \\beta} | + r_{\\alpha}$\n",
        "- $\\tilde{\\matr{W}} := \\beta \\matr{I}$, where $\\beta$ is a scalar, and $\\matr{I}$ is the identity matrix of size $d$\n",
        "\n",
        "Notice that this variant also features the non-linearity $\\sigma$.  Strictly speaking, this is not a gradient flow anymore. Nevertheless, the energy is decreasing along the solution of the gradient flow equation if $\\sigma$ satisfies $x \\sigma(x) \\geq 0$.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Uoqwcg9vJMkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's implement the \"external field\" layer. A naive way to implement the multiplication $\\matr{F} \\cdot \\operatorname{diag}(\\matr{\\omega})$ would be to explicitly construct matrix $\\operatorname{diag}(\\matr{\\omega})$ and multiply it with $\\matr{F}$. However, it is easy to see that this is equivalent to the elementwise multiplication of each row in $\\matr{F}$ with the row vector $\\matr{\\omega}^{\\intercal}$. Therefore, we can efficiently implement it by leveraging broadcasting."
      ],
      "metadata": {
        "id": "SuYDVx8QpNiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class External(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.weight = torch.nn.Parameter(torch.empty((1, num_features)))\n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.normal_(self.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.weight"
      ],
      "metadata": {
        "id": "xWQdJ64hVPMh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we implement the multiplication with $\\matr{W}$ as a linear layer which we wrap inside the `Pairwise` class. \n",
        "\n",
        "Here, $\\matr{W}$ has a special structure which must be preserved during training. To account for this, we use PyTorch's [parametrization](https://pytorch.org/tutorials/intermediate/parametrizations.html) functionality. The `forward` method of `PairwiseParametrization` is a function of weight that imposes symmetry and a diagonally dominant structure. \n",
        "- Symmetry can be imposed by taking the sum of the upper-triangular part of the matrix and its transpose. \n",
        "- Next, the main diagonal is constructed with the help of additional parameters $\\matr{q}$ and $\\matr{r}$ which we store in the last two columns. \n",
        "\n",
        "We then add this parametrization to the linear layer with `parametrize.register_parametrization`. Under the hood, `PairwiseParametrization` gets invoked during each forward pass."
      ],
      "metadata": {
        "id": "7Bxvewv5rIfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PairwiseParametrization(torch.nn.Module):\n",
        "    def forward(self, W):\n",
        "        # Construct a symmetric matrix with zero diagonal\n",
        "        W0 = W[:, :-2].triu(1)\n",
        "        W0 = W0 + W0.T\n",
        "\n",
        "        # Retrieve the `q` and `r` vectors from the last two columns\n",
        "        q = W[:, -2]\n",
        "        r = W[:, -1]\n",
        "        # Construct the main diagonal\n",
        "        w_diag = torch.diag(q * torch.sum(torch.abs(W0), 1) + r) \n",
        "\n",
        "        return W0 + w_diag\n",
        "\n",
        "\n",
        "class Pairwise(torch.nn.Module):\n",
        "    def __init__(self, num_hidden):\n",
        "        super().__init__()\n",
        "        # Pay attention to the dimensions\n",
        "        self.lin = torch.nn.Linear(num_hidden + 2, num_hidden, bias=False)\n",
        "        # Add parametrization\n",
        "        parametrize.register_parametrization(self.lin, \"weight\", PairwiseParametrization(), unsafe=True)\n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        self.lin.reset_parameters()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.lin(x)"
      ],
      "metadata": {
        "id": "DsoBx6n5BVhU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we multiply the initial condition value $\\matr{F}(0)$ with a scalar $\\beta$. Although it is trivial, we again implement it as a module just to stick to the convention."
      ],
      "metadata": {
        "id": "TsAT9dsvR6r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Source(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.weight = torch.nn.Parameter(torch.empty(1))\n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.normal_(self.weight)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x * self.weight"
      ],
      "metadata": {
        "id": "GYzrCrmuR3is"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can write our GRAFF network ðŸ¦’. It would follow the architecture of our previous GCN network whenever it is possible.\n",
        "\n",
        "Points to consider:\n",
        "\n",
        "- Fixed step size $0 < \\tau \\leq 1$, and it does not have to be equal $1$ as in GCN (We pass the step size as an argument)\n",
        "\n",
        "- We need to project the raw node features onto the subspace of dimensionality $d$ (*which is the dimension of square matrices in our convolution*) with the **encoder** (in this tutorial, we implement it as a linear layer)\n",
        "\n",
        "- Similarly, the **decoder** projects the output of the last convolutional layer onto the subspace of dimensionality $k$ (*the number of classes*)\n",
        "\n",
        "- Don't forget the residual connections!\n",
        "\n",
        "- We use ReLU as the non-linearity, as it satisfies $x \\sigma(x) \\geq 0$\n"
      ],
      "metadata": {
        "id": "GXPRfIZrJcFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRAFFNet(torch.nn.Module):\n",
        "    def __init__(self, dataset, num_hidden, self_loops=True, step_size=1.):\n",
        "        super().__init__()\n",
        "        self.step_size = step_size\n",
        "\n",
        "        # Encoder\n",
        "        self.enc = torch.nn.Linear(dataset.num_features, num_hidden, bias=False)\n",
        "\n",
        "        # Initialize the linear layers\n",
        "        self.ext_lin = External(num_hidden)\n",
        "        self.pair_lin = Pairwise(num_hidden)\n",
        "        self.source_lin = Source()\n",
        "\n",
        "        # Initialize the GRAFF layer\n",
        "        self.conv = GRAFFConv(self.ext_lin, self.pair_lin, self.source_lin, self_loops=self_loops)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec = torch.nn.Linear(num_hidden, dataset.num_classes, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        self.enc.reset_parameters()\n",
        "        self.ext_lin.reset_parameters()\n",
        "        self.pair_lin.reset_parameters()\n",
        "        self.source_lin.reset_parameters()\n",
        "        self.dec.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        \n",
        "        # Apply the encoder\n",
        "        x = self.enc(x)\n",
        "        # Copy the initial features\n",
        "        x0 = x.clone()\n",
        "\n",
        "        # This context manager caches the parametrization to reduce redundant calculations\n",
        "        with parametrize.cached():\n",
        "            x = x + self.step_size * F.relu(self.conv(x, edge_index, x0))\n",
        "            x = x + self.step_size * F.relu(self.conv(x, edge_index, x0))\n",
        "\n",
        "        # Apply the decoder\n",
        "        x = self.dec(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "1eNXHbB4SOaX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiments\n",
        "\n",
        "We consider the task of node classification on a single graph (transductive setting). The datasets are `Texas` (*low homophily*) from `Cora` (*high homophily*). The train/val/test masks are taken from the [Geom-GCN paper](https://arxiv.org/pdf/2002.05287.pdf) (10 random splits)\n",
        "\n",
        "First, let us examine the datasets."
      ],
      "metadata": {
        "id": "jxROxx6T_ZWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_dataset(dataset):\n",
        "    print(f'Dataset name:', dataset.name)\n",
        "    \n",
        "    runs = dataset[0]['train_mask'].shape[1]\n",
        "    print(f'Number of splits in dataset: {runs}')\n",
        "\n",
        "    print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "    print(f'Number of nodes: {dataset[0].num_nodes}')\n",
        "    print(f'Number of edges: {dataset[0].num_edges}')\n",
        "\n",
        "    h = homophily(dataset[0].edge_index, dataset[0].y)\n",
        "    print(f'Homophily: {h:.3f}')"
      ],
      "metadata": {
        "id": "EjVD5dnMz1zG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_texas = WebKB(root='/tmp/Texas', name='Texas')"
      ],
      "metadata": {
        "id": "ykMs7AjSyWKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_cora = Planetoid(root='/tmp/Cora', name='Cora', split='Geom-GCN')"
      ],
      "metadata": {
        "id": "dtAAxBs7zjJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_dataset(dataset_texas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHQSV2Rkz_KN",
        "outputId": "45604f64-fc5f-49f4-eb88-b20215bc26ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset name: texas\n",
            "Number of splits in dataset: 10\n",
            "Number of classes: 5\n",
            "Number of nodes: 183\n",
            "Number of edges: 325\n",
            "Homophily: 0.108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_dataset(dataset_cora)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnjY0_Fg0DYU",
        "outputId": "c5b26cb2-e01c-4ab4-e208-72b8b043b871"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset name: Cora\n",
            "Number of splits in dataset: 10\n",
            "Number of classes: 7\n",
            "Number of nodes: 2708\n",
            "Number of edges: 10556\n",
            "Homophily: 0.810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important qualitative property of a graph is homophily. In the [Geom-GCN paper](https://arxiv.org/pdf/2002.05287.pdf), it is defined as the average fraction of neighbors with the same label:\n",
        "\n",
        "$$h = \\frac{1}{|V|} \\sum_{v \\in V} \\frac{\\textrm{Number of }v\\textrm{'s neighbors who have the same label as }v}{\\textrm{Number of }v\\textrm{'s neighbors}} $$\n",
        "\n",
        "It can be computed with a PyG function `torch_geometric.utils.homophily`. As can be seen from the summarization above, `Texas` is heterophilic, whereas `Cora` is homophilic.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zMPeFa3r1ysp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us write the training and evaluation pipeline. We run our pipeline on each of the 10 splits. Each run is terminated with early stopping monitored on the validation set (when neither loss nor accuracy is improved over the last 100 epochs). The quality of the models is assessed on the test set of each split, and the accuracy is evaluated when the validation loss is minimal. For training, we use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer without a learning rate scheduler."
      ],
      "metadata": {
        "id": "7hDYkZdGp1eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, data, split):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask[:,split]], data.y[data.train_mask[:,split]])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data, split):\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    outs = {}\n",
        "\n",
        "    for key in ['train', 'val', 'test']:\n",
        "        mask = data[f'{key}_mask'][:,split]\n",
        "        loss = float(F.nll_loss(out[mask], data.y[mask]))\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        outs[f'{key}_loss'] = loss\n",
        "        outs[f'{key}_acc'] = acc\n",
        "\n",
        "    return outs\n",
        "\n",
        "\n",
        "def run_train(dataset, model, runs, epochs, lr, weight_decay, early_stopping):\n",
        "    val_losses, accs, durations = [], [], []\n",
        "    # Each run corresponds to a different train/val/test split\n",
        "    for run in range(runs):\n",
        "        data = dataset[0]\n",
        "        data = data.to(device)\n",
        "        \n",
        "        # Suppress warnings regarding masks\n",
        "        data.train_mask = data.train_mask.bool()\n",
        "        data.val_mask = data.val_mask.bool()\n",
        "        data.test_mask = data.test_mask.bool()\n",
        "\n",
        "        model.to(device).reset_parameters()\n",
        "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_start = time.perf_counter()\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_val_acc = 0\n",
        "        cur_step = 0\n",
        "\n",
        "        test_acc = 0\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            # Training and evaluation\n",
        "            train(model, optimizer, data, split=run)\n",
        "            eval_info = evaluate(model, data, split=run)\n",
        "            eval_info['epoch'] = epoch\n",
        "\n",
        "            # Best test acc logging and early stop logic\n",
        "            if eval_info['val_loss'] < best_val_loss:\n",
        "                best_val_loss = eval_info['val_loss']\n",
        "                test_acc = eval_info['test_acc']\n",
        "                cur_step = 0\n",
        "            elif eval_info['val_acc'] > best_val_acc:\n",
        "                best_val_acc = eval_info['val_acc']\n",
        "                cur_step = 0\n",
        "            else:\n",
        "                cur_step += 1\n",
        "\n",
        "            if cur_step >= early_stopping:\n",
        "                print(f'Training terminated on epoch {epoch}')\n",
        "                break\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_end = time.perf_counter()\n",
        "\n",
        "        val_losses.append(best_val_loss)\n",
        "        accs.append(test_acc)\n",
        "        durations.append(t_end - t_start)\n",
        "    loss, acc, duration = torch.tensor(val_losses), torch.tensor(accs), torch.tensor(durations)\n",
        "\n",
        "    print(f'Val Loss: {float(loss.mean()):.4f}, '\n",
        "          f'Test Accuracy: {float(acc.mean()):.3f} Â± {float(acc.std()):.3f}, '\n",
        "          f'Duration: {float(duration.mean()):.3f}s')"
      ],
      "metadata": {
        "id": "pwPIkshN638x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = dataset_texas[0]['train_mask'].shape[1]\n",
        "epochs = 5000\n",
        "num_hidden = 64\n",
        "lr = 1e-3\n",
        "weight_decay = 5e-6\n",
        "early_stopping = 100"
      ],
      "metadata": {
        "id": "Dqd7nZQ1-vx6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCNNet(dataset_texas, num_hidden)\n",
        "\n",
        "run_train(dataset_texas, model, runs, epochs, lr, weight_decay, early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHEajLwEBZQN",
        "outputId": "c3e59480-a1f0-48e7-8726-3b4915498248"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training terminated on epoch 117\n",
            "Training terminated on epoch 116\n",
            "Training terminated on epoch 124\n",
            "Training terminated on epoch 119\n",
            "Training terminated on epoch 123\n",
            "Training terminated on epoch 114\n",
            "Training terminated on epoch 103\n",
            "Training terminated on epoch 117\n",
            "Training terminated on epoch 139\n",
            "Training terminated on epoch 125\n",
            "Val Loss: 1.3577, Test Accuracy: 0.543 Â± 0.181, Duration: 0.971s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRAFFNet(dataset_texas, num_hidden, self_loops=False, step_size=0.5)\n",
        "\n",
        "run_train(dataset_texas, model, runs, epochs, lr, weight_decay, early_stopping)"
      ],
      "metadata": {
        "id": "tY_ZbY3gEADX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3568b03-5483-43d0-f875-e16da6ce7459"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training terminated on epoch 138\n",
            "Training terminated on epoch 243\n",
            "Training terminated on epoch 166\n",
            "Training terminated on epoch 147\n",
            "Training terminated on epoch 163\n",
            "Training terminated on epoch 140\n",
            "Training terminated on epoch 121\n",
            "Training terminated on epoch 141\n",
            "Training terminated on epoch 121\n",
            "Training terminated on epoch 143\n",
            "Val Loss: 0.6392, Test Accuracy: 0.749 Â± 0.058, Duration: 1.063s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runs = dataset_cora[0]['train_mask'].shape[1]\n",
        "epochs = 5000\n",
        "num_hidden = 64\n",
        "lr = 1e-3\n",
        "weight_decay = 5e-5\n",
        "early_stopping = 100"
      ],
      "metadata": {
        "id": "9DO0tYSTkeb1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCNNet(dataset_cora, num_hidden)\n",
        "\n",
        "run_train(dataset_cora, model, runs, epochs, lr, weight_decay, early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a402e156-a705-4790-ca2f-b046816c93be",
        "id": "i-43eDTilTRo"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training terminated on epoch 276\n",
            "Training terminated on epoch 270\n",
            "Training terminated on epoch 277\n",
            "Training terminated on epoch 271\n",
            "Training terminated on epoch 314\n",
            "Training terminated on epoch 279\n",
            "Training terminated on epoch 290\n",
            "Training terminated on epoch 299\n",
            "Training terminated on epoch 265\n",
            "Training terminated on epoch 281\n",
            "Val Loss: 0.4231, Test Accuracy: 0.859 Â± 0.017, Duration: 1.448s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRAFFNet(dataset_cora, num_hidden, self_loops=False, step_size=0.25)\n",
        "\n",
        "run_train(dataset_cora, model, runs, epochs, lr, weight_decay, early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a91f3e5a-9117-4bfc-ff22-1268832fda15",
        "id": "nqFcUBXklTRq"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training terminated on epoch 548\n",
            "Training terminated on epoch 180\n",
            "Training terminated on epoch 163\n",
            "Training terminated on epoch 154\n",
            "Training terminated on epoch 153\n",
            "Training terminated on epoch 139\n",
            "Training terminated on epoch 133\n",
            "Training terminated on epoch 132\n",
            "Training terminated on epoch 125\n",
            "Training terminated on epoch 124\n",
            "Val Loss: 0.4185, Test Accuracy: 0.862 Â± 0.016, Duration: 1.282s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conclude, GRAFF performs well on both extreme settings (low/high homophily). On Texas dataset, it substantially outperforms a vanilla GCN.  However, on Cora, the performance of GRAFF is on par with GCN.\n",
        "\n",
        "You can further use this notebook to experiment with hyperparameters, such as `step_size` and `num_hidden`. One way to improve the current GRAFF implementation is substituting the linear encoder and decoder with MLPs."
      ],
      "metadata": {
        "id": "mHxLXY5pEw4o"
      }
    }
  ]
}